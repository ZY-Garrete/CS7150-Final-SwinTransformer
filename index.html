<!doctype html>
<html lang="en">

<head>
  <title>A Comparative Study of CNNs and Swin Transformers on Traffic Sign Classification</title>
  <meta property="og:title" content=  Swin vs CNNs Transformers on Traffic Signs" />
  <meta name="twitter:title" content="  Swin vs CNNs Transformers on Traffic Signs" />
  <meta name="description" content="Comparative study of Swin Transformers and CNNs using GTSRB dataset for CS7150." />
  <meta property="og:description" content="Comparative study of Swin Transformers and CNNs using GTSRB dataset for CS7150." />
  <meta name="twitter:description" content="Comparative study of Swin Transformers and CNNs using GTSRB dataset for CS7150." />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <!-- bootstrap for mobile-friendly layout -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
    integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
    integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
    crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">

</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <h1 class="lead">
        <nobr class="widenobr">A Comparative Study of CNNs and Swin Transformers on Traffic Sign Classification</nobr>
        <nobr class="widenobr">For CS 7150</nobr>
      </h1>
    </div>
  </div><!-- end nd-pageheader -->

  <div class="container">
    <div class="row">
      <div class="col justify-content-center text-center">
        <h2>An Analysis of "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"</h2>
        <p>This project investigates whether the Swin Transformer can outperform traditional CNNs like ResNet-50 in
          real-world image classification tasks. Using the GTSRB traffic sign dataset, we evaluate both models under the
          same training conditions to ensure a fair comparison. Our core question is whether vision transformers like
          Swin provide tangible benefits over conventional architectures. Additionally, we explore how the size of the
          attention window in Swin affects its performance, aiming to understand whether tuning this key parameter can
          further enhance its ability to recognize complex visual patterns.</p>
      </div>
    </div>
    <div class="row">
      <div class="col">

        <section id="analysis-section" style="display: flex; flex-direction: column; gap: 2rem; padding: 2rem;">

          <!-- Private Investigator -->
          <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem;">
            <h3>Private Investigator</h3>
            <img src="images/ze_liu.png" alt="Ze Liu" style="width: 200px; border-radius: 8px; margin: 1rem 0;">
            <p><strong>Ze Liu</strong><br>
              Researcher at Microsoft Research Asia (MSRA)<br>
              Focus: Visual Representation Learning, Hierarchical Modeling</p>
            <p>Swin Transformer was proposed by a group of leading researchers at MSRA. Their goal was to design a
              general-purpose hierarchical vision transformer that can scale effectively across classification,
              detection, and segmentation tasks. The baseline CNN, ResNet, was also originally developed at MSRA.</p>
          </div>

          <!-- Diagrammer -->
          <!-- <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem;">
            <h3>Diagrammer</h3>
            <img src="images/swin_vs_resnet.png" alt="Swin vs ResNet"
              style="width: 100%; border-radius: 8px; margin: 1rem 0;">
            <p>This diagram compares Swin Transformer’s hierarchical structure with the plain structure of ResNet. Swin
              uses shifted windows to compute local attention efficiently, while ResNet processes spatial data using
              convolutional kernels.</p>
          </div> -->

          <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem;">
            <h3>Diagrammer</h3>
          
            <!-- 图1：Swin vs ViT -->
            <div style="margin-bottom: 2rem;">
              <img src="images/diagram1.png" style="width: 100%; border-radius: 8px;">
              <p><strong>Figure 1:</strong> Comparison of hierarchical Swin Transformer vs. flat ViT.</p>
              <p>Unlike ViT which applies global attention at a fixed scale, Swin Transformer processes images in a hierarchical fashion—starting with small patches and gradually merging them. This enables Swin to support not only classification but also detection and segmentation.</p>
            </div>
          
            <!-- 图2：Shifted Window Attention -->
            <div style="margin-bottom: 2rem;">
              <img src="images/diagram2.png" style="width: 100%; border-radius: 8px;">
              <p><strong>Figure 2:</strong> Shifted window self-attention across layers.</p>
              <p>Swin Transformer performs local attention within windows in one layer, and then shifts the windows in the next. This allows information exchange between neighboring regions while keeping the computation efficient.</p>
            </div>
          
            <!-- 图3：Patch Partitioning & Merging -->
            <div style="margin-bottom: 2rem;">
              <img src="images/diagram3.png" style="width: 100%; border-radius: 8px;">
              <p><strong>Figure 3:</strong> Patch partition and merging across stages.</p>
              <p>The model begins by partitioning the image into small patches and applies a linear embedding. As it progresses through the stages, patches are merged and representations are updated hierarchically, mimicking a CNN-like resolution pyramid.</p>
            </div>
          
            <!-- 图4：Swin Block Architecture -->
            <div style="margin-bottom: 2rem;">
              <img src="images/diagram4.png" style="width: 100%; border-radius: 8px;">
              <p><strong>Figure 4:</strong> Inner workings of a Swin Transformer Block.</p>
              <p>Each block contains a Window-based Multi-head Self-Attention (W-MSA) and a Shifted Window MSA (SW-MSA), combined with Layer Normalization and MLP layers. This design enables both efficiency and local-global context capture.</p>
            </div>
          
            <p style="font-size: small; color: gray;">Figures based on Liu et al., 2021 (ICCV): "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows".</p>
          </div>
          

          <!-- Reviewer -->
          <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem;">
            <h3>Reviewer</h3>

            <p><strong>Review Score:</strong> <span style="font-weight: bold; font-size: 1.1rem;">7/10 Accept</span></p>
            <p style="font-style: italic;">
              Swin Transformer, a unified backbone architecture that scales across vision tasks through hierarchical
              design
              and efficient windowed attention.
            </p>

            <h4 style="margin-top: 1rem;">Strengths:</h4>
            <ul style="padding-left: 1.2rem;">
              <li>
                Introduces an efficient <strong>shifted window mechanism</strong>, significantly reducing attention
                complexity while maintaining representation power.
              </li>
              <li>
                Strong empirical results across classification, detection, and segmentation benchmarks.
              </li>
              <li>
                Modular design easily extendable to future vision applications.
              </li>
            </ul>

            <h4 style="margin-top: 1rem;">Weaknesses:</h4>
            <ul style="padding-left: 1.2rem;">
              <li>Swin may not feel highly innovative compared to other vision architectures.</li>
              <li>Lacks theoretical justification for shift size or merging patterns.</li>
            </ul>
          </div>

          <!-- Experimenter -->
          <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem;">
            <h3>Experiment</h3>
            <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem; margin-bottom: 2rem;">
              <h3>Dataset: GTSRB</h3>
            
              <p>
                The <strong>German Traffic Sign Recognition Benchmark (GTSRB)</strong> is a widely-used dataset for multi-class image classification,
                designed for developing and evaluating traffic sign recognition models. It contains over <strong>50,000 images</strong> covering
                <strong>43 traffic sign classes</strong>, with variations in scale, lighting, rotation, and occlusion—making it a challenging benchmark for real-world vision systems.
              </p>
            
              <ul>
                <li><strong>Image count:</strong> 51,839</li>
                <li><strong>Classes:</strong> 43 different traffic signs</li>
                <li><strong>Image size:</strong> Varies, mostly resized to 32×32 or 64×64 in preprocessing</li>
                <li><strong>Applications:</strong> Used in autonomous driving, traffic sign recognition, visual robustness testing</li>
              </ul>
            
              <div style="text-align: center; margin-top: 1rem;">
                <img src="images/dataset.png" alt="GTSRB sample" style="max-width: 100%; border-radius: 8px; box-shadow: 0 0 8px rgba(0,0,0,0.1);">
                <p style="font-size: 0.9rem; color: gray;">Figure: Sample images from the GTSRB dataset showing various traffic signs under different lighting and angles.</p>
              </div>
            </div>
            




            <p>We implemented Swin-Tiny and ResNet-50 on the GTSRB dataset under the same conditions. The Swin
              Transformer outperformed ResNet in accuracy and convergence. We also investigated how changing the Swin
              window size affects model performance, with surprising insights on pattern sensitivity.</p>
            <img src="images/training results_swintrans.png" alt="GTSRB Results"
              style="width: 100%; border-radius: 8px; margin: 1rem 0;">

              <div style="margin-bottom: 2rem;">
                <h4>Accuracy Comparison</h4>
                <div style="display: flex; flex-wrap: wrap; gap: 1rem;">
                  <div style="flex: 1;">
                    <img src="images/Accuracy of CNN.png" alt="CNN Accuracy" style="width: 100%; border-radius: 8px;">
                    <p style="font-size: 0.9rem;">Training and validation accuracy of ResNet-50 (CNN). It shows fast convergence and high validation accuracy approaching 99% after just a few epochs.</p>
                  </div>
                  <div style="flex: 1;">
                    <img src="images/Accuracy of Swin-Transformer.png" alt="Swin Transformer Accuracy" style="width: 100%; border-radius: 8px;">
                    <p style="font-size: 0.9rem;">Accuracy curve of Swin Transformer shows stable improvements, reaching ~98.3% accuracy, with less variance after epoch 5. Suggests good generalization and robustness.</p>
                  </div>
                </div>
              </div>
            
              <!-- Loss Comparison -->
              <div style="margin-bottom: 2rem;">
                <h4>Loss Comparison</h4>
                <div style="display: flex; flex-wrap: wrap; gap: 1rem;">
                  <div style="flex: 1;">
                    <img src="images/Loss of CNN.jpg" alt="CNN Loss" style="width: 100%; border-radius: 8px;">
                    <p style="font-size: 0.9rem;">Loss curves of ResNet-50 show consistent decline, and low validation loss after epoch 3, indicating strong fit to data with low overfitting risk.</p>
                  </div>
                  <div style="flex: 1;">
                    <img src="images/Loss of Swin-Transformer.png" alt="Swin Transformer Loss" style="width: 100%; border-radius: 8px;">
                    <p style="font-size: 0.9rem;">Loss curve of Swin Transformer also decreases steadily. While not as steep initially, it stabilizes well, suggesting a strong capacity for noise-tolerant optimization.</p>
                  </div>
                </div>
              </div>
            
              <p>Overall, both models perform well, but Swin Transformer shows comparable or slightly better generalization ability. Its hierarchical architecture and shifted windows contribute to stable convergence with fewer parameters.</p>
          </div>

        </section>
        <h3>References</h3>

        <p><a name="bottou-1990">[1]</a>
          <a href="https://arxiv.org/abs/2103.14030" target="_blank">
            Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021).
            <em>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</em>.
            In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 10012–10022.
          </a>
        </p>

        <h2>Team Members</h2>

        <p>Zefeng Zhao, Zuyu Guo.</p>

        <!-- Acknowledgment -->
        <h2>Acknowledgment</h2>
        <p>
          We would like to express our sincere gratitude to
          <strong><a href="https://baulab.info/" target="_blank"
              style="text-decoration: none; color: #007acc;">Professor David Bau</a></strong>
          for his guidance throughout this course. His thoughtful lectures and insightful discussions helped us better
          understand modern deep learning models and inspired us to explore the capabilities of Swin Transformer in a
          real-world vision task.
        </p>



      </div><!--col-->
    </div><!--row -->
  </div> <!-- container -->

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col-6 col-md text-center">
        <a href="https://cs7150.baulab.info/">About CS 7150</a>
      </div>
    </div>
  </footer>

</body>
<script>
  $(document).on('click', '.clickselect', function (ev) {
    var range = document.createRange();
    range.selectNodeContents(this);
    var sel = window.getSelection();
    sel.removeAllRanges();
    sel.addRange(range);
  });
  // Google analytics below.
  window.dataLayer = window.dataLayer || [];
</script>

</html>
