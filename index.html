<!doctype html>
<html lang="en">

<head>
  <title>A Comparative Study of CNNs and Swin Transformers on Traffic Sign Classification</title>
  <meta property="og:title" content=Swin vs CNNs Transformers on Traffic Signs" />
  <meta name="twitter:title" content="  Swin vs CNNs Transformers on Traffic Signs" />
  <meta name="description" content="Comparative study of Swin Transformers and CNNs using GTSRB dataset for CS7150." />
  <meta property="og:description"
    content="Comparative study of Swin Transformers and CNNs using GTSRB dataset for CS7150." />
  <meta name="twitter:description"
    content="Comparative study of Swin Transformers and CNNs using GTSRB dataset for CS7150." />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <!-- bootstrap for mobile-friendly layout -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
    integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
    integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
    crossorigin="anonymous"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <style>
    p.math {
      text-align: center;
      font-family: "Courier New", monospace;
      font-size: 1.05rem;
    }
  </style>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">

</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <h1 class="lead">
        <nobr class="widenobr">A Comparative Study of CNNs and Swin Transformers on Traffic Sign Classification</nobr>
        <nobr class="widenobr">For CS 7150</nobr>
      </h1>
    </div>
  </div><!-- end nd-pageheader -->

  <div class="container">
    <div class="row">
      <div class="col justify-content-center text-center">
        <h2>An Analysis of "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"</h2>
        <p>
          The Swin Transformer paper introduces a powerful and scalable vision architecture that extends the success of
          transformers to a wide range of computer vision tasks.
          Unlike standard Vision Transformers (ViT), which use global self-attention, Swin employs a hierarchical design
          with non - overlapping windows, making it more computationally efficient.
          The key innovation lies in the <strong>shifted window mechanism</strong>, which allows cross-window
          connections and improves information flow across local regions.
        </p>
        <p>
          The architecture progresses through four stages, gradually reducing spatial dimensions via <strong>Patch
            Merging</strong> and increasing the number of channels - mirroring the design philosophy of CNNs.
          This allows the model to capture fine-grained features in earlier stages and more abstract representations in
          deeper layers.
        </p>
        <p>
          The paper demonstrates strong results on benchmark datasets in image classification, object detection, and
          semantic segmentation, showing the Swin Transformer’s potential as a general - purpose backbone.
          Its modular nature and competitive performance suggest a promising alternative to traditional convolutional
          networks.
        </p>
      </div>
    </div>
    <div class="row">
      <div class="col">

        <section id="analysis-section" style="display: flex; flex-direction: column; gap: 2rem; padding: 2rem;">

          <!-- Private Investigator -->
          <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1.5rem;">
            <h3>Private Investigator</h3>

            <div style="display: flex; flex-wrap: wrap; gap: 1.5rem; justify-content: space-around;">
              <!-- Ze Liu -->
              <div style="width: 220px; text-align: center;">
                <img src="images/ze_liu.png" alt="Ze Liu"
                  style="width: 100%; border-radius: 8px; margin-bottom: 0.5rem;">
                <strong>Ze Liu</strong><br>
                Microsoft Research Asia (MSRA)<br>
                Focus: Vision, Hierarchical Modeling
              </div>

              <!-- Han Hu -->
              <div style="width: 220px; text-align: center;">
                <img src="images/han_hu.jpg" alt="Han Hu"
                  style="width: 100%; border-radius: 8px; margin-bottom: 0.5rem;">
                <strong>Han Hu</strong><br>
                MSRA<br>
                Focus: 3D Vision, Transformers
              </div>

              <!-- yue_cao -->
              <div style="width: 220px; text-align: center;">
                <img src="images/yue_cao.jpg" alt="yue_cao"
                  style="width: 100%; border-radius: 8px; margin-bottom: 0.5rem;">
                <strong>Yue Cao</strong><br>
                MSRA<br>
                Focus: Vision Research Leadership
              </div>

              <!-- Yutong Lin -->
              <div style="width: 220px; text-align: center;">
                <strong>Yutong Lin</strong><br>
                MSRA<br>
                Focus: Efficient Visual Learning
              </div>
            </div>

            <p style="margin-top: 1.5rem;">
              Swin Transformer was proposed by a group of leading researchers at Microsoft Research Asia (MSRA).
              Their goal was to design a general-purpose hierarchical vision transformer that scales effectively across
              classification, detection, and segmentation tasks. The baseline CNN, ResNet, was also originally developed
              at MSRA.
            </p>
          </div>

          <!-- Diagrammer -->
          <!-- <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem;">
            <h3>Diagrammer</h3>
            <img src="images/swin_vs_resnet.png" alt="Swin vs ResNet"
              style="width: 100%; border-radius: 8px; margin: 1rem 0;">
            <p>This diagram compares Swin Transformer’s hierarchical structure with the plain structure of ResNet. Swin
              uses shifted windows to compute local attention efficiently, while ResNet processes spatial data using
              convolutional kernels.</p>
          </div> -->

          <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem;">
            <h3>Diagrammer</h3>

            <!-- image1：Swin vs ViT -->
            <div style="margin-bottom: 2rem;">
              <img src="images/diagram1.png" style="width: 100%; border-radius: 8px;">
              <p><strong>Figure 1:</strong> Comparison of hierarchical Swin Transformer vs. flat ViT.</p>
              <p>Unlike ViT which applies global attention at a fixed scale, Swin Transformer processes images in a
                hierarchical fashion—starting with small patches and gradually merging them. This enables Swin to
                support not only classification but also detection and segmentation.</p>
            </div>

            <!-- image2：Shifted Window Attention -->
            <div style="margin-bottom: 2rem;">
              <img src="images/diagram2.png" style="width: 100%; border-radius: 8px;">
              <p><strong>Figure 2:</strong> Shifted window self-attention across layers.</p>
              <p>Swin Transformer performs local attention within windows in one layer, and then shifts the windows in
                the next. This allows information exchange between neighboring regions while keeping the computation
                efficient.</p>
            </div>

            <!-- image3：The overall architecture of the Swin transformer -->
            <!-- <div style="margin-bottom: 2rem;">
              <img src="images/diagram3.png" style="width: 100%; border-radius: 8px;">
              <p><strong>Figure 3:</strong> The overall architecture of the Swin transformer.</p>
              <p>The model adopts a hierarchical design with four stages in total. Except for the first stage, each
                subsequent stage begins with a Patch Merging layer that downsamples the input feature map, reducing its
                resolution. This allows the model to progressively enlarge the receptive field-similar to how CNNs
                operate-enabling it to capture increasingly global information.</p>
              <p>
                At the beginning of the model, a <strong>Patch Partition</strong> operation is applied-this is
                equivalent to the <strong>Patch Embedding</strong> in ViT. The input image is split into non-overlapping
                patches using a convolutional layer with a patch size of 4, and these patches are embedded into vectors
                with a dimension of 48 (similar to how tokens are embedded in NLP). In the first stage, a <strong>Linear
                  Embedding</strong> layer adjusts the channel dimension to <strong>C</strong>. Each subsequent stage
                (except the first) begins with a <strong>Patch Merging</strong> module, which downsamples the feature
                map resolution, allowing the model to increase its receptive field like CNNs. Within each stage,
                multiple <strong>Swin Transformer Blocks</strong> are applied. As shown in the diagram, each block
                consists of <strong>Layer Normalization</strong>, <strong>Window-based Multi-Head Self-Attention
                  (W-MSA)</strong>, <strong>Shifted Window Attention (SW-MSA)</strong>, and a <strong>Multilayer
                  Perceptron (MLP)</strong>.
              </p>

            </div> -->
            <div style="margin-bottom: 2rem;">
              <img src="images/diagram3.png" style="width: 100%; border-radius: 8px;">
              <p><strong>Figure 3:</strong> The overall architecture of the Swin Transformer.</p>

              <p>
                The model adopts a hierarchical design with four stages in total. Except for the first stage, each
                subsequent stage begins with a
                <strong>Patch Merging</strong> layer that downsamples the input feature map, reducing its resolution.
                This allows the model to
                progressively enlarge the receptive field - similar to how CNNs operate - enabling it to capture
                increasingly global information.
              </p>

              <p>
                At the beginning of the model, a <strong>Patch Partition</strong> operation is applied—this is
                equivalent to the <strong>Patch Embedding</strong> in ViT.
                The input image of size \( H \times W \times 3 \) is divided into non-overlapping patches of size \( 4
                \times 4 \), producing
                \( \frac{H}{4} \times \frac{W}{4} \) patches, each with a flattened vector of dimension \( 4 \times 4
                \times 3 = 48 \).
                Thus, the embedded feature sequence has shape:
              </p>

              <p class="math">
                \( \mathbf{X}_0 \in \mathbb{R}^{\frac{H}{4} \cdot \frac{W}{4} \times 48} \)
              </p>

              <p>
                In the first stage, a <strong>Linear Embedding</strong> layer projects these 48-dimensional vectors into
                a hidden dimension \( C \), where:
              </p>

              <p class="math">
                \( \mathbf{X}_1 = \mathbf{X}_0 \cdot \mathbf{W}_{\text{embed}} \in \mathbb{R}^{\frac{H}{4} \cdot
                \frac{W}{4} \times C} \)
              </p>

              <p>
                Each subsequent stage (except the first) starts with <strong>Patch Merging</strong>, which concatenates
                features from a 2×2 region and applies a linear projection:
              </p>

              <p class="math">
                \( \text{PatchMerging}(x) = \text{Linear}(\text{Concat}(x_{i,j}, x_{i,j+1}, x_{i+1,j}, x_{i+1,j+1})) \)
              </p>

              <p>
                This reduces the resolution by a factor of 2 and doubles the channel dimension, producing a more
                abstract and compact representation for deeper stages.
              </p>

              <p>
                Inside each stage, several <strong>Swin Transformer Blocks</strong> are applied. As shown in the
                diagram, each block contains:
              </p>
              <ul>
                <li><strong>Layer Normalization</strong> before each major operation</li>
                <li><strong>Window-based Multi-head Self-Attention (W-MSA)</strong> computing attention within
                  non-overlapping windows</li>
                <li><strong>Shifted Window Attention (SW-MSA)</strong> to enable cross-window connections</li>
                <li><strong>Multilayer Perceptron (MLP)</strong> with two linear layers and a GELU activation</li>
              </ul>

              <p>
                The W-MSA and SW-MSA operations can be formally described as:
              </p>

              <p class="math">
                \( \text{Attention}(Q, K, V) = \text{Softmax}\left( \frac{QK^\top}{\sqrt{d}} \right)V \)
              </p>

              <p>
                where \( Q, K, V \) are linear projections of the input \( X \) within each window and \( d \) is the
                dimensionality of the keys.
              </p>
            </div>

            <!-- image4：Swin Block Architecture -->
            <div style="margin-bottom: 2rem;">
              <img src="images/diagram4.png" style="width: 100%; border-radius: 8px;">
              <p><strong>Figure 4:</strong> Understanding the framework from a code perspective.</p>
              <p>
                In the Swin Transformer architecture, <strong>Patch Merging</strong> is placed at the end of each stage.
                The input first goes through a series of <strong>Swin Transformer Blocks</strong> for feature
                extraction, followed by downsampling via Patch Merging. The final stage does not perform any further
                downsampling. Instead, its output is passed directly to a fully connected layer, where it is used to
                compute the loss against the target labels.
              </p>
              <pre><code class="language-python">
              # window_size=7 
              # input_batch_image.shape=[128,3,224,224]
              class SwinTransformer(nn.Module):
                  def __init__(...):
                      super().__init__()
                      ...
                      # absolute position embedding
                      if self.ape:
                          self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
              
                      self.pos_drop = nn.Dropout(p=drop_rate)
              
                      # build layers
                      self.layers = nn.ModuleList()
                      for i_layer in range(self.num_layers):
                          layer = BasicLayer(...)
                          self.layers.append(layer)
              
                      self.norm = norm_layer(self.num_features)
                      self.avgpool = nn.AdaptiveAvgPool1d(1)
                      self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
              
                  def forward_features(self, x):
                      x = self.patch_embed(x) # Patch Partition
                      if self.ape:
                          x = x + self.absolute_pos_embed
                      x = self.pos_drop(x)
              
                      for layer in self.layers:
                          x = layer(x)
              
                      x = self.norm(x)  # Batch_size Windows_num Channels
                      x = self.avgpool(x.transpose(1, 2))  # Batch_size Channels 1
                      x = torch.flatten(x, 1)
                      return x
              
                  def forward(self, x):
                      x = self.forward_features(x)
                      x = self.head(x) # self.head => Linear(in=Channels,out=Classification_num)
                      return x
              </code></pre>

            </div>

            <!-- image5：CNNss Architecture -->
            <div style="margin-bottom: 2rem;">
              <img src="images/diagram5.png" style="width: 100%; border-radius: 8px;">
              <p><strong>Figure 5:</strong> The overall architecture of the CNNs.</p>
              <p>
                We implemented a Convolutional Neural Network (CNN) to classify traffic signs into 43 categories using
                the GTSRB dataset.
                The input is a 32 * 32 RGB image passed through five convolutional stages. Each stage uses 3 * 3 filters,
                ReLU activation, batch normalization,
                and dropout (25%) to prevent overfitting. Max pooling is applied to reduce spatial dimensions,
                eventually down to 4 * 4.
              </p>
              <p>
                After feature extraction, the output is flattened and passed through a fully connected layer with 128
                units and 50% dropout.
                The final layer outputs a 43-dimensional vector, transformed into class probabilities using softmax.
              </p>
              <p>
                The model was trained for 15 epochs with a batch size of 32. It combines strong feature extraction,
                dimensionality reduction, and regularization,
                making it well-suited for robust traffic sign recognition.
              </p>
            </div>

            <p style="font-size: small; color: gray;">Figures based on Liu et al., 2021 (ICCV): "Swin Transformer:
              Hierarchical Vision Transformer using Shifted Windows".</p>
          </div>


          <!-- Reviewer -->
          <!-- <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem;">
            <h3>Reviewer</h3>

            <p><strong>Review Score:</strong> <span style="font-weight: bold; font-size: 1.1rem;">7/10 Accept</span></p>
            <p style="font-style: italic;">
              Swin Transformer, a unified backbone architecture that scales across vision tasks through hierarchical
              design
              and efficient windowed attention.
            </p>

            <h4 style="margin-top: 1rem;">Strengths:</h4>
            <ul style="padding-left: 1.2rem;">
              <li>
                Introduces an efficient <strong>shifted window mechanism</strong>, significantly reducing attention
                complexity while maintaining representation power.
              </li>
              <li>
                Strong empirical results across classification, detection, and segmentation benchmarks.
              </li>
              <li>
                Modular design easily extendable to future vision applications.
              </li>
            </ul>

            <h4 style="margin-top: 1rem;">Weaknesses:</h4>
            <ul style="padding-left: 1.2rem;">
              <li>Swin may not feel highly innovative compared to other vision architectures.</li>
              <li>Lacks theoretical justification for shift size or merging patterns.</li>
            </ul>
          </div> -->
          <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem;">
            <h3 style="font-size: 1.4rem; margin-bottom: 0.5rem;">Reviewer Comments</h3>

            <p><strong>Overall Recommendation:</strong>
              <span style="font-weight: bold; font-size: 1.15rem; color: #006400;">Accept (7/10)</span>
            </p>

            <p style="font-style: italic; margin-bottom: 1.5rem;">
              Swin Transformer presents a unified and scalable vision backbone, designed around hierarchical modeling
              and
              computationally efficient window-based self-attention.
            </p>

            <h4 style="margin: 1rem 0 0.3rem;">Strengths</h4>
            <ul style="padding-left: 1.2rem; margin-top: 0;">
              <li>
                <strong>Shifted Window Attention</strong> offers a clever balance between local and global context
                modeling,
                dramatically improving efficiency without sacrificing accuracy.
              </li>
              <li>
                Demonstrates <strong>state-of-the-art performance</strong> across diverse tasks—image classification,
                object detection, and semantic segmentation.
              </li>
              <li>
                The architecture is <strong>modular and extensible</strong>, making it well-suited for future
                advancements in
                vision-based AI systems.
              </li>
            </ul>

            <h4 style="margin: 1rem 0 0.3rem;">Weaknesses</h4>
            <ul style="padding-left: 1.2rem; margin-top: 0;">
              <li>
                The paper lacks a <strong>deeper theoretical analysis</strong> of the shifted window strategy and its
                limitations.
              </li>
              <li>
                Compared to other emerging transformer variants, Swin's conceptual novelty might be perceived as more
                incremental.
              </li>
            </ul>
          </div>


          <!-- Experimenter -->
          <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem;">
            <h3>Experiment</h3>
            <div style="border: 1px solid #ccc; border-radius: 12px; padding: 1rem; margin-bottom: 2rem;">
              <h3>Dataset: GTSRB</h3>

              <p>
                The <strong>German Traffic Sign Recognition Benchmark (GTSRB)</strong> is a widely-used dataset for
                multi-class image classification,
                designed for developing and evaluating traffic sign recognition models. It contains over <strong>50,000
                  images</strong> covering
                <strong>43 traffic sign classes</strong>, with variations in scale, lighting, rotation, and
                occlusion—making it a challenging benchmark for real-world vision systems.
              </p>

              <ul>
                <li><strong>Image count:</strong> 51,839</li>
                <li><strong>Classes:</strong> 43 different traffic signs</li>
                <li><strong>Image size:</strong> Varies, mostly resized to 32*32 or 64*64 in preprocessing</li>
                <li><strong>Applications:</strong> Used in autonomous driving, traffic sign recognition, visual
                  robustness testing</li>
              </ul>

              <div style="text-align: center; margin-top: 1rem;">
                <img src="images/dataset.png" alt="GTSRB sample"
                  style="max-width: 100%; border-radius: 8px; box-shadow: 0 0 8px rgba(0,0,0,0.1);">
                <p style="font-size: 0.9rem; color: gray;">Figure: Sample images from the GTSRB dataset showing various
                  traffic signs under different lighting and angles.</p>
              </div>
            </div>





            <p>We implemented Swin-Tiny and ResNet-50 on the GTSRB dataset under the same conditions. The Swin
              Transformer outperformed ResNet in accuracy and convergence. We also investigated how changing the Swin
              window size affects model performance, with surprising insights on pattern sensitivity.</p>
            <img src="images/training results_swintrans.png" alt="GTSRB Results"
              style="width: 100%; border-radius: 8px; margin: 1rem 0;">

            <div style="margin-bottom: 2rem;">
              <h4>Accuracy Comparison</h4>
              <div style="display: flex; flex-wrap: wrap; gap: 1rem;">
                <div style="flex: 1;">
                  <img src="images/Accuracy of CNN.png" alt="CNN Accuracy" style="width: 100%; border-radius: 8px;">
                  <p style="font-size: 0.9rem;">Training and validation accuracy of ResNet-50 (CNN). It shows fast
                    convergence and high validation accuracy approaching 97% after just a few epochs.</p>
                </div>
                <div style="flex: 1;">
                  <img src="images/Accuracy of Swin-Transformer.png" alt="Swin Transformer Accuracy"
                    style="width: 100%; border-radius: 8px;">
                  <p style="font-size: 0.9rem;">Accuracy curve of Swin Transformer shows stable improvements, reaching
                    ~98.3% accuracy, with less variance after epoch 5. Suggests good generalization and robustness.</p>
                </div>
              </div>
            </div>

            <!-- Loss Comparison -->
            <div style="margin-bottom: 2rem;">
              <h4>Loss Comparison</h4>
              <div style="display: flex; flex-wrap: wrap; gap: 1rem;">
                <div style="flex: 1;">
                  <img src="images/Loss of CNN.jpg" alt="CNN Loss" style="width: 100%; border-radius: 8px;">
                  <p style="font-size: 0.9rem;">Loss curves of ResNet-50 show consistent decline, and low validation
                    loss after epoch 3, indicating strong fit to data with low overfitting risk.</p>
                </div>
                <div style="flex: 1;">
                  <img src="images/Loss of Swin-Transformer.png" alt="Swin Transformer Loss"
                    style="width: 100%; border-radius: 8px;">
                  <p style="font-size: 0.9rem;">Loss curve of Swin Transformer also decreases steadily. While not as
                    steep initially, it stabilizes well, suggesting a strong capacity for noise-tolerant optimization.
                  </p>
                </div>
              </div>
            </div>

            <p>Overall, both models perform well, but Swin Transformer shows comparable or slightly better
              generalization ability. Its hierarchical architecture and shifted windows contribute to stable convergence
              with fewer parameters.</p>

            <div style="margin-top: 2rem; padding: 1rem; background-color: #f9f9f9; border-left: 4px solid #007acc;">
              <h3>Download Our Trained Model</h3>
              <p>
                We provide the best-performing Swin Transformer model trained on the GTSRB dataset.
                If you'd like to run your own experiments or verify our results, you can download the model from the
                link below:
              </p>
              <p>
                 <a href="https://drive.google.com/file/d/1pLvKtfuMXS5sMzP_bIoDb4DWmycnvv9_/view?usp=sharing"
                  target="_blank" style="font-weight: bold; color: #007acc;">
                  Download Swin - Tranformer Weight file (.pth) [with top 1 accuracy reaches 98.3 %]
                </a>
              </p>
              <p>
                After downloading, you can place the `.pth` file in Swin - Transformer code part at project folder /Swin - Transformer - main / output and load it using PyTorch for
                inference or further training.
              </p>
            </div>
          </div>

        </section>
        <h3>References</h3>

        <p><a name="bottou-1990">[1]</a>
          <a href="https://arxiv.org/abs/2103.14030" target="_blank">
            Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021).
            <em>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</em>.
            In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 10012–10022.
          </a>
        </p>
        <p><a name="bottou-1990">[2]</a>
          <a href="https://arxiv.org/abs/2005.12872" target="_blank">
            Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. (2020, August).
            <em>End-to-end
              object detection with transformers</em>. In European conference on computer vision (pp. 213-229). Cham:
            Springer
            International Publishing.
          </a>
        </p>
        <p><a name="bottou-1990">[3]</a>
          <a href="https://www.nature.com/articles/nature14539" target="_blank">
            LeCun, Y., Bengio, Y., & Hinton, G. (2015). <em> Deep learning </em>. nature, 521(7553), 436-444.
          </a>
        </p>
        <p><a name="bottou-1990">[4]</a>
          <a href="https://arxiv.org/abs/1409.1556" target="_blank">
            Simonyan, K., & Zisserman, A. (2014).<em> Very deep convolutional networks for large-scale image
              recognition</em>.
            arXiv preprint arXiv:1409.1556.
          </a>
        </p>


        <h2>Team Members</h2>

        <p>Zefeng Zhao, Zuyu Guo.</p>

        <!-- Acknowledgment -->
        <h2>Acknowledgment</h2>
        <p>
          We would like to express our sincere gratitude to
          <strong><a href="https://baulab.info/" target="_blank"
              style="text-decoration: none; color: #007acc;">Professor David Bau</a></strong>
          for his guidance throughout this course. His thoughtful lectures and insightful discussions helped us better
          understand modern deep learning models and inspired us to explore the capabilities of Swin Transformer in a
          real-world vision task.
        </p>



      </div><!--col-->
    </div><!--row -->
  </div> <!-- container -->

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col-6 col-md text-center">
        <a href="https://cs7150.baulab.info/">About CS 7150</a>
      </div>
    </div>
  </footer>

</body>
<script>
  $(document).on('click', '.clickselect', function (ev) {
    var range = document.createRange();
    range.selectNodeContents(this);
    var sel = window.getSelection();
    sel.removeAllRanges();
    sel.addRange(range);
  });
  // Google analytics below.
  window.dataLayer = window.dataLayer || [];
</script>

</html>
